/**
 * Created: 22 Aug 2014
 */
package mapreduce.guardedfragment.planner.compiler.reducers;

import java.io.Serializable;
import java.util.Collection;
import java.util.HashSet;
import java.util.Set;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.io.Text;

import mapreduce.guardedfragment.planner.compiler.mappers.GFMapper1Iterator;
import mapreduce.guardedfragment.planner.structures.data.Tuple;
import mapreduce.guardedfragment.planner.structures.operations.GFOperationInitException;
import mapreduce.guardedfragment.planner.structures.operations.GFReducer;
import mapreduce.guardedfragment.structure.gfexpressions.GFAtomicExpression;
import mapreduce.guardedfragment.structure.gfexpressions.GFExistentialExpression;
import mapreduce.guardedfragment.structure.gfexpressions.io.Pair;
import mapreduce.guardedfragment.structure.gfexpressions.operations.GFAtomProjection;
import mapreduce.guardedfragment.structure.gfexpressions.operations.NonMatchingTupleException;

/**
 * Uses atom data generated by the corresponding mapper.
 * 
 * @author Jonny Daenen
 * 
 */
public class GFReducer1AtomBased extends GFReducer implements Serializable {

	private static final long serialVersionUID = 1L;
	private final static String FILENAME = "tmp_round1_red.txt";

	private static final Log LOG = LogFactory.getLog(GFReducer1AtomBased.class);

	/**
	 * @throws GFOperationInitException
	 * @see mapreduce.guardedfragment.planner.structures.operations.GFReducer#reduce(java.lang.String,
	 *      java.lang.Iterable)
	 */
	@Override
	// / OPTIMIZE iterable string?
	public HashSet<Pair<Text, String>> reduce(String key, Iterable<? extends Object> values)
			throws GFOperationInitException {

		HashSet<Pair<Text, String>> result = new HashSet<Pair<Text, String>>();
		Set<Text> buffer = new HashSet<>();
		
		boolean keyFound = false;
		
		for (Object v : values) {
			
			// WARNING Text object will be reused by Hadoop!
			Text t = (Text) v;
			
			// is this a guard
			if ( t.find(";") >= 0) {
				
				// if the key has already been found, we can just output 
				// TODO this is mainly for when we turn this thing into an iterator
				if (keyFound) {
					result.add(new Pair<>(t,FILENAME));
				}
				// else we collect the data
				else {
					// create new object because Text object will be reduce by Hadoop
					buffer.add(new Text(t));
				}
				
				
			// if this is the key, we mark it
			} else if (!keyFound) {
				keyFound = true;
			}
		}
		
		// output the remaining data
		if (keyFound) {
			for (Text p : buffer) {
				result.add(new Pair<>(p,FILENAME));
			}
		}

	
		
		return result;
	}



}
