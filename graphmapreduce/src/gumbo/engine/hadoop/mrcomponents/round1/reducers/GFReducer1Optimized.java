/**
 * Created: 22 Aug 2014
 */
package gumbo.engine.hadoop.mrcomponents.round1.reducers;

import java.io.IOException;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import gumbo.engine.general.algorithms.Red1Algorithm;
import gumbo.engine.general.messagefactories.Red1MessageFactoryInterface;
import gumbo.engine.hadoop.mrcomponents.round1.algorithms.Red1MessageFactory;
import gumbo.engine.hadoop.mrcomponents.tools.ParameterPasser;
import gumbo.engine.hadoop.settings.HadoopExecutorSettings;
import gumbo.structures.gfexpressions.operations.ExpressionSetOperations;

/**
 * Uses atom data generated by the corresponding mapper.
 * 
 * @author Jonny Daenen
 * 
 */
public class GFReducer1Optimized extends Reducer<Text, Text, Text, Text> {

	private final static String FILENAME = "round1";


	private static final Log LOG = LogFactory.getLog(GFReducer1Optimized.class);


	private Red1Algorithm algo;


	/**
	 * @see org.apache.hadoop.mapreduce.Mapper#setup(org.apache.hadoop.mapreduce.Mapper.Context)
	 */
	@Override
	protected void setup(Context context) throws IOException, InterruptedException {
		// load context
		super.setup(context);

		String s = String.format("Reducer"+this.getClass().getSimpleName()+"-%05d-%d",
				context.getTaskAttemptID().getTaskID().getId(),
				context.getTaskAttemptID().getId());
		LOG.info(s);



		// load parameters
		try {
			Configuration conf = context.getConfiguration();

			ParameterPasser pp = new ParameterPasser(conf);
			ExpressionSetOperations eso = pp.loadESO();
			HadoopExecutorSettings settings = pp.loadSettings();

			Red1MessageFactoryInterface msgFactory = new Red1MessageFactory(context, settings, eso, FILENAME);
			algo = new Red1Algorithm(eso,settings,msgFactory);

		} catch (Exception e) {
			LOG.error(e.getMessage());
			e.printStackTrace();
			throw new InterruptedException("Reducer initialisation error: " + e.getMessage());
		}

	}

	@Override
	protected void cleanup(Context context) throws IOException, InterruptedException {
		try {
			algo.cleanup();
		} catch(Exception e) {
			e.printStackTrace();
			LOG.error(e.getMessage());
			throw new InterruptedException(e.getMessage());
		}
	}

	/**
	 * @see org.apache.hadoop.mapreduce.Reducer#reduce(java.lang.Object,
	 *      java.lang.Iterable, org.apache.hadoop.mapreduce.Reducer.Context)
	 */
	@Override
	protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {

		try {
			
			algo.initialize(key.toString());

			// WARNING Text object will be reused by Hadoop!
			for (Text t : values) {

				// the following is faster then first converting to a String representation
				if(!algo.processTuple(algo.split(t.getBytes(), t.getLength())))
					break;
				
			}

			// indicate end of tuples
			// and finish calculation
			algo.finish();

		} catch(Exception e) {
			e.printStackTrace();
			LOG.error(e.getMessage());
			throw new InterruptedException(e.getMessage());
		}
		
		

	}

	



}
